\documentclass[14pt]{extarticle}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}


\title{
\huge{\textbf{ Using Reinforcement Learning to Train a fucking chicken\\[1 cm]}}
\includegraphics[scale=0.5]{avatar.png}}
\author{Maximilian Herzogschläger \and Hans Profesoor Olischläger \and Philipp Tepelschläger}


\usepackage{mathtools}  % lädt »amsmath«

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\maketitle

\pagebreak

\section{Introduction}

The given Bomberman game is obviously (as discussed in the lecture) a Markov model, which is fully observable and thus our policy, on which our agent acts, fulfills the prerequisites of a Marcov decision process. In the earlier stages of the project (the first two tasks without enemies) we have a deterministic world in theory. %%Macht Sinn??

This means we could have used a model to learn our world and the 'transition probabilities' (delta functions) to train our agent on this world. However, when thinking about the tournament where we would have other agents acting on their own accord, the world loses the deterministic property in our agents perspective. This is why we went for a model free Q learning approach from the get go, because in the end we would end up with a world non deterministic transition probabilities and also would not want to learn those. We also do not need to understand what our agent is doing as long as it is beating the others, so being a blackbox seemed to be a small disadvantage here.

??The first Q learning approach that came to our minds is of course the 'exploration-exploitation trade-off', which we discussed in the lecture.??

To help us with evaluating our agent's learning process, we wrote a script 'analysis.py', which allows us to monitor different variables during the training process, i.e. the total rewards our agent gained throughout each single episode. This is one of the methods we used to rate the learning success or rate, but we will explain more on that, in the respective tasks where the script was used.
\newpage
\noindent
The common hyperparameters used in this report will be:
\begin{itemize}
    \item $\epsilon$ : The probability of choosing a random action in the $\epsilon$ greedy algorithm
    \item $\alpha$ : The arbitrary learning rate we use when updating our action value function in Q learning
    \item $\gamma$ : The discount factor that determines how much significance we give to the subsequent gamestates when updating our action value function in Q learning
\end{itemize}

\pagebreak

\section{Task 1: Collecting Coins in an Open Labyrinth}
Before even thinking about setting features, hyperparameters or rewards for collecting coins in an open maze, we had to decide which method of reinforcement learning we would go with. After being overwhelmed by the sheer amount of possibilities in the beginning, we tried to settle for the simplest approach given in the lecture: The $\epsilon$ greedy policy. So our basic algorithm was going to be:
\begin{itemize}
    \item{Make an initial guess for our matrix Q %%MATRIX ODER TENSOR?
        which gives the agent an easier entrance to the learning process}
    \item{Let the agent play an episode where in every move it plays the highest value action in the current state (meaning with the given features) with a probability of $1 -\epsilon$ and does a random (sometimes weighted) action with a probability $\epsilon$}
    \item{When an episode is over, we evaluate every combination of action and gamestate we encountered in the specific episode and compare our action value function $Q$ with the actual rewards earned to adapt our $Q$ accordingly}
    \item Repeat this until the desired success is achieved by the agent
\end{itemize}

After understanding this basic setup, we started thinking about 

\newpage
\textbf{Moin!} Ich hab meine erlebnisse beim ausprobieren hier ziemlich ungefiltert reinschmiert. Ich denke mal sowas wie eine beispielhafte behandlung von unseren daily struggles würde Sinn ergeben im Report?
\section{Hyperparameter learning (Context is regression with LVA (LVE?)}
Implementing a new algorithmic approach doesn't lead to results right away. In order to confirm a working implementation one seeks to see learning results. However, there is some time needed to tune the hyperparameters that enable learning.
The learning setup is characterized by the algorithm, its learning parameters and the rewards that the agent ought to use as purpose of each action.
To be specific, [I use right now] 4 step temporal difference Q learning with regression by linear value approximation. I just copy the json here to save time...  \\
\\
"ALPHA": 0.2,\\
"GAMMA": 0.9,\\
"EPSILON": 0.2,\\
"EXP\_BUFFER\_SIZE": 2,\\
"N": 4,\\
"GAME\_REWARDS": \\
"COIN\_COLLECTED": 1,\\
"INVALID\_ACTION": -0.2,\\
"CRATE\_DESTROYED": 0.5,\\
"KILLED\_SELF": -1,\\
"WAITED": -0.2,\\
"NEW\_PLACE": 0.1\\
CRATE\_DENSITY is 0.4\\
\newpage
When choosing initial values one has to rely on guessing. By repeatedly training and observing the results we can conjecture what went wrong and build an intuition for reasonable auxiliary rewards. We observed, that the agent can easily get stuck in a local optimum. In finding good rewards we are faced with two main challenges:
\textbf{The final game rewards are to sparse}
If rewards occur very rarely most of the training time is lost by random behavior that doesn't give any feedback for updating the model. Most actions lead to nothing.
\textbf{Auxiliary rewards lead to ultimately unwanted behavior}
Auxiliary rewards meant to nudge the model towards a desired behavior can instead alter the behavior in an unforeseen way.
We have to rely on auxiliary rewards to give our model enough feedback to learn, so we have to try to minimize the biases that come with them.
As the auxiliary rewards are meant to nudge rather than to repurpose the agent we could call the unwanted behavior \textit{auxiliary reward abuse} (??? what u think ???)

I suspect it might be helpful to only use auxiliary rewards in the early stages of training and drop them in the end, but this will have to be investigated later.

At an early stage, we can directly observe said \textit{auxiliary reward abuse}. For example going for a somewhat minimal approach proved to be unsatisfactory. Rewarding only destroyed crates and collected coins while punishing invalid actions and killing itself was thought to be a good initial idea, because it would give some auxiliary rewards, but only for events with undoubted value in any game regardless of specific situation.
However, being punished repeatedly for trying to move or placing bombs, the agent soon learned to do nothing and just wait.
We tried two things:
Punishing waiting lead to an agent tending to walk back and forth unproductively. This is a more sophisticated waiting agent and fundamentally suffers from the same flaw.
The next trial was to reward the agent to visit new places. [As I write this I am not sure anymore if this was dumb, Im not sure anymore if this changes anything, u got an idea?]

This seemed to provide a good enough basis for a longer training session, so set it up to train 30000 rounds and regularly checked in to see what the agent was doing in between.

During the training the agent appeared to have learned a specific move, namely placing a bomb, going around a corner and waiting there. In certain settings this strategy can  very effectively destroy crates. However, oftentimes the agent fails doing this, because it tries to move into a wall where it hoped to find a hiding place. Then the bomb goes off and it dies.
Overall, it rarely works out, so it should be punished by the learning process and eventually adapt to become more reliable or vanish completely. In this case it the behavior vanished.

Overall, the agents actions appeared to be more diverse as before the rewards had been tweaked (specifically the visit new places reward), but it still showed both the waiting and stepping back and forth

It is suspected, that a higher epsilon (0.3) a lower N (3) a lower punishment for self kill (-0.1)[due to the fact that a small test proved to frighten the agent to drop bombs at all] and a additional reward for placed bombs (0.1) would be better.

Nobomb punishment makes a lot of sense, because it specifically punishes boring waiters and hopefully nudges towards better exploration. This should work as it is worse to live 150 steps than to try to bomb a crate and die in the process.

hypothesis: nstep td with high N makes more sense for longer training sessions, because it otherwise relies on those random exploration actions that don't get integrated in Q...


Good night my lovely diary ;)



\end{document}
